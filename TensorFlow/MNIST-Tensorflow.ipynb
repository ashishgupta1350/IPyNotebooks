{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets('MNIST_data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fa5eced9748>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fa5af7460f0>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fa5af746208>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (55000, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape,mnist.train.labels.shape # Not one hot coded now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape,mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 784), (5000, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.validation.images.shape,mnist.validation.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After doing one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (55000, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape,mnist.train.labels.shape # Not one hot coded now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape,mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 784), (5000, 10))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.validation.images.shape,mnist.validation.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[0] # It is a 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "first_image=mnist.train.images[1]\n",
    "first_image=np.array(first_image,dtype='float')\n",
    "first_image=first_image.reshape((28,28)) # reshape in 28*28 pixels\n",
    "plt.imshow(first_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_arr=np.random.random((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa5aebe8198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[18].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.base import get_data_home \n",
    "print (get_data_home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #print(\n",
    "        tf.random_normal([784,256]).eval()\n",
    "    #)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input=784\n",
    "n_hidden_1=256\n",
    "n_hidden_2=256\n",
    "n_classes=10\n",
    "# Each of the 784 features going into each unit of hidden layer 1 unit\n",
    "weights={\n",
    "    'h1':tf.Variable(tf.random_normal([n_input,n_hidden_1])),\n",
    "    'h2':tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal([n_hidden_2,n_classes]))\n",
    "}\n",
    "# The bias (1 additional feature) is going into each of the hidden layer 1 unit\n",
    "biases={\n",
    "    'h1':tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'h2':tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(x,weights,biases):\n",
    "    in_layer1=tf.add(tf.matmul(x,weights['h1']),biases['h1']) # This is the net input going into layer 1\n",
    "    out_layer1=tf.nn.relu(in_layer1) # Applying this activation function\n",
    "    \n",
    "    in_layer2=tf.add(tf.matmul(out_layer1,weights['h2']),biases['h2'])\n",
    "    out_layer2=tf.nn.relu(in_layer2)\n",
    "    \n",
    "    output=tf.add(tf.matmul(out_layer2,weights['out']),biases['out'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding predictions and accuracy with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\",[None,n_input]) # None because we are not sure of how many data points and n_input because each data point has 784 features\n",
    "y = tf.placeholder(tf.int32,[None,n_classes]) # None because we are not sure of how many data points we are passing and n_classes because we have to predict from 10 available classes\n",
    "pred = forward_propogation(x,weights,biases) # X can be either training data or testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating cost using cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(784, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_2:0' shape=(256, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_3:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_4:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_5:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels = y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer first decides on what variables does cost depend upon which is weights and biases which further depend upon h1,h2,out in both the layers\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "optimize = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) # This will initialize all the tensorflow variable once again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.20092\n",
      "72.18972\n",
      "68.877075\n",
      "65.6104\n",
      "62.774292\n",
      "60.571274\n",
      "58.95807\n",
      "57.799553\n",
      "56.850155\n",
      "55.862865\n",
      "54.657494\n",
      "53.20949\n",
      "51.605156\n",
      "50.007114\n",
      "48.550556\n",
      "47.278656\n",
      "46.191906\n",
      "45.20008\n",
      "44.22488\n",
      "43.221783\n",
      "42.188843\n",
      "41.171\n",
      "40.18971\n",
      "39.29714\n",
      "38.520386\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    # Every time the weights and biases are updated ,the complete data set is feeded (stochastic gradient descent)\n",
    "    c,_ = sess.run([cost,optimize], feed_dict={x:mnist.train.images , y:mnist.train.labels})\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 95.64 %\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum in y for each data point , because that is the digit that I am going to predict\n",
    "predictions = tf.arg_max(pred,1)\n",
    "correct_labels = tf.arg_max(y,1)\n",
    "correct_predictions = tf.equal(predictions,correct_labels)\n",
    "predictions_evaluated,labels,correct_pred = sess.run([predictions,correct_labels,correct_predictions],feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "# feed_dict only need x as predictions depend only upon x and not on any other parameter\n",
    "predictions_evaluated,labels,correct_pred # False mean wrong and True means right\n",
    "print(\"Accuracy is :\" ,correct_pred.sum()/len(mnist.test.labels) * 100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(25):\n",
    "    # Every time the weights and biases are updated ,the complete data set is feeded (stochastic gradient descent)\n",
    "    c,_ = sess.run([cost,optimize], feed_dict={x:mnist.train.images , y:mnist.train.labels})\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11743.095062971115\n",
      "3609.660050109029\n",
      "2210.122366058547\n",
      "1397.8416607348704\n",
      "1100.600599762373\n",
      "1032.816126814321\n",
      "964.7961507570318\n",
      "819.5742163836018\n",
      "727.9790352943602\n",
      "657.6668760176685\n",
      "512.999181450055\n",
      "509.7214386775686\n",
      "419.78280051062984\n",
      "474.79585947359914\n",
      "329.726447637582\n",
      "353.7880389226131\n",
      "304.4687035769716\n",
      "295.74674930718373\n",
      "239.36496720287937\n",
      "182.7181067114854\n",
      "196.51661575811355\n",
      "194.41506871553995\n",
      "140.60204642375885\n",
      "141.54089442385245\n",
      "108.24533117159055\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "for i in range(25):\n",
    "    num_batches=int(mnist.train.num_examples/batch_size)\n",
    "    total_cost = 0\n",
    "    for j in range(num_batches):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "        c,_=sess.run([cost,optimize],feed_dict={x:batch_x,y:batch_y})\n",
    "        total_cost+=c\n",
    "    print(total_cost) # total cost seems to be higher at each point ,since we are summing multiple costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
